{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f675f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global steps: 500\n",
      "Memory size: 500\n",
      "Drives (curiosity, competence): 0.07730855883817105 5.249395600387454e-05\n"
     ]
    }
   ],
   "source": [
    "# Add the current directory to Python path to find the brain package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the directory containing this notebook to Python path\n",
    "# This allows importing the 'brain' package that's in the same directory\n",
    "notebook_dir = Path(globals()[\"__vsc_ipynb_file__\"]).parent.resolve()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Import from the structured package\n",
    "from brain import (\n",
    "    BrainAgent,\n",
    "    MultiModalEncoder,\n",
    "    MultiModalDummyEnv,\n",
    "    build_and_run_demo\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Run the demo\n",
    "if __name__ == \"__main__\":\n",
    "    build_and_run_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5128fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading agent from brain_tictactoe_agent.pkl...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtic_tac_toe_notebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TicTacToeNotebook\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the game interface (optionally load existing agent)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# If 'brain_tictactoe_agent.pkl' exists, it will be loaded automatically (trained tabular MC agent)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Otherwise, specify agent_path to load a different agent, or leave None to create a new one\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m game \u001b[38;5;241m=\u001b[39m TicTacToeNotebook(\n\u001b[1;32m      9\u001b[0m     agent_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrain_tictactoe_agent.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Load trained tabular MC agent, or None for new agent\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtictactoe_agent.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Save path for new training\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Option 1: Use interactive widgets (recommended)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Click \"Start Training\" to have the agent play against itself\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Click \"Start Continuous Play\" to play multiple games with a human\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Click \"Exit\" to stop and save the agent\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ui \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mcreate_widget_ui()\n",
      "File \u001b[0;32m~/Projects/jupyter/Brain/tic_tac_toe_notebook.py:32\u001b[0m, in \u001b[0;36mTicTacToeNotebook.__init__\u001b[0;34m(self, agent_path, save_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent_path \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(agent_path):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading agent from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m BrainAgent\u001b[38;5;241m.\u001b[39mload(agent_path)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Disable intrinsic motivation for Tic Tac Toe (it adds noise)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mintrinsic\u001b[38;5;241m.\u001b[39mcuriosity_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Projects/jupyter/Brain/brain/agent.py:208\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39mupdate_monte_carlo(episode, winner_id)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episode\n",
      "File \u001b[0;32m~/Projects/jupyter/Brain/brain/agent.py:275\u001b[0m, in \u001b[0;36mfrom_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuromodulators\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    268\u001b[0m     reward\u001b[38;5;241m=\u001b[39mtotal_reward,\n\u001b[1;32m    269\u001b[0m     value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    270\u001b[0m     next_value\u001b[38;5;241m=\u001b[39mnext_value,\n\u001b[1;32m    271\u001b[0m     pred_error_norm\u001b[38;5;241m=\u001b[39mpred_error_norm,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Store transition\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(x, z, action, total_reward, x_next, z_next, done)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m td_error, pred_error_norm, intrinsic_reward, drive_vec\n",
      "File \u001b[0;32m~/Projects/jupyter/Brain/brain/actor_critic.py:92\u001b[0m, in \u001b[0;36mfrom_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W_policy'"
     ]
    }
   ],
   "source": [
    "# Tic Tac Toe with Learning Agent\n",
    "# Import the notebook-friendly interface\n",
    "from tic_tac_toe_notebook import TicTacToeNotebook\n",
    "\n",
    "# Create the game interface (optionally load existing agent)\n",
    "# If 'brain_tictactoe_agent.pkl' exists, it will be loaded automatically (trained tabular MC agent)\n",
    "# Otherwise, specify agent_path to load a different agent, or leave None to create a new one\n",
    "game = TicTacToeNotebook(\n",
    "    agent_path='brain_tictactoe_agent.pkl',  # Load trained tabular MC agent, or None for new agent\n",
    "    save_path='tictactoe_agent.pkl'  # Save path for new training\n",
    ")\n",
    "\n",
    "# Option 1: Use interactive widgets (recommended)\n",
    "# Click \"Start Training\" to have the agent play against itself\n",
    "# Click \"Start Continuous Play\" to play multiple games with a human\n",
    "# Click \"Exit\" to stop and save the agent\n",
    "ui = game.create_widget_ui()\n",
    "display(ui)\n",
    "\n",
    "# Option 2: Simple text-based continuous play (works in any notebook)\n",
    "# Uncomment to play continuously (type 'q' to quit):\n",
    "# game.play_simple(agent_first=True, continuous=True)\n",
    "\n",
    "# Option 3: Train agent by self-play (text-based)\n",
    "# Uncomment to train the agent:\n",
    "# game.train_self_play(n_games=100)\n",
    "\n",
    "# Show stats\n",
    "game.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa9becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final board: XOX OXX OXO\n",
      "Result: draw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "EMPTY = \".\"\n",
    "MARKS = [\"X\", \"O\"]\n",
    "\n",
    "# ---------- basic board utilities ----------\n",
    "\n",
    "WIN_LINES = [\n",
    "    (0, 1, 2),\n",
    "    (3, 4, 5),\n",
    "    (6, 7, 8),\n",
    "    (0, 3, 6),\n",
    "    (1, 4, 7),\n",
    "    (2, 5, 8),\n",
    "    (0, 4, 8),\n",
    "    (2, 4, 6),\n",
    "]\n",
    "\n",
    "\n",
    "def check_winner(board):\n",
    "    \"\"\"Return 'X', 'O', 'draw', or None.\"\"\"\n",
    "    for a, b, c in WIN_LINES:\n",
    "        if board[a] != EMPTY and board[a] == board[b] == board[c]:\n",
    "            return board[a]\n",
    "    if EMPTY not in board:\n",
    "        return \"draw\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- symmetries and canonical states ----------\n",
    "\n",
    "# index maps: new_board[i] = old_board[MAP[i]]\n",
    "IDX = list(range(9))\n",
    "\n",
    "ROT90 = [6, 3, 0, 7, 4, 1, 8, 5, 2]\n",
    "ROT180 = [8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "ROT270 = [2, 5, 8, 1, 4, 7, 0, 3, 6]\n",
    "REF_H = [2, 1, 0, 5, 4, 3, 8, 7, 6]\n",
    "REF_V = [6, 7, 8, 3, 4, 5, 0, 1, 2]\n",
    "REF_MAIN = [0, 3, 6, 1, 4, 7, 2, 5, 8]\n",
    "REF_ANTI = [8, 5, 2, 7, 4, 1, 6, 3, 0]\n",
    "\n",
    "SYMMETRIES = [\n",
    "    IDX,\n",
    "    ROT90,\n",
    "    ROT180,\n",
    "    ROT270,\n",
    "    REF_H,\n",
    "    REF_V,\n",
    "    REF_MAIN,\n",
    "    REF_ANTI,\n",
    "]\n",
    "\n",
    "\n",
    "def flip_marks(board):\n",
    "    \"\"\"Swap X and O, keep EMPTY.\"\"\"\n",
    "    res = []\n",
    "    for c in board:\n",
    "        if c == \"X\":\n",
    "            res.append(\"O\")\n",
    "        elif c == \"O\":\n",
    "            res.append(\"X\")\n",
    "        else:\n",
    "            res.append(c)\n",
    "    return \"\".join(res)\n",
    "\n",
    "\n",
    "def canonical(board, mark_to_move):\n",
    "    \"\"\"\n",
    "    Return (canonical_board, transform).\n",
    "\n",
    "    canonical_board is the lexicographically smallest symmetric variant\n",
    "    after possibly flipping marks so that the player to move is always 'X'.\n",
    "    transform[i] gives the index in the original board that corresponds\n",
    "    to canonical_board[i].\n",
    "    \"\"\"\n",
    "    if mark_to_move == \"X\":\n",
    "        base = board\n",
    "    else:\n",
    "        base = flip_marks(board)\n",
    "\n",
    "    best = None\n",
    "    best_map = None\n",
    "\n",
    "    for M in SYMMETRIES:\n",
    "        cand = \"\".join(base[M[i]] for i in range(9))\n",
    "        if best is None or cand < best:\n",
    "            best = cand\n",
    "            best_map = M\n",
    "\n",
    "    return best, best_map\n",
    "\n",
    "\n",
    "# ---------- RL: Q table and policy ----------\n",
    "\n",
    "class TicTacToeLearner:\n",
    "    def __init__(self):\n",
    "        # Q[state][action] = expected return for player to move\n",
    "        self.Q = {}\n",
    "        self.N = {}  # visit counts for step size 1/N\n",
    "\n",
    "    def _ensure_state(self, state):\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = [0.0] * 9\n",
    "            self.N[state] = [0] * 9\n",
    "\n",
    "    def choose_action(self, board, mark, epsilon):\n",
    "        \"\"\"Epsilon greedy from canonical state. Returns (real_action_index, state_key, action_in_canonical).\"\"\"\n",
    "        canon, mapping = canonical(board, mark)\n",
    "        self._ensure_state(canon)\n",
    "\n",
    "        # find available actions in canonical board\n",
    "        avail_can = [i for i, c in enumerate(canon) if c == EMPTY]\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            a_can = random.choice(avail_can)\n",
    "        else:\n",
    "            qvals = self.Q[canon]\n",
    "            a_can = max(avail_can, key=lambda a: qvals[a])\n",
    "\n",
    "        # map canonical action to real board index\n",
    "        a_real = mapping[a_can]\n",
    "        return a_real, canon, a_can\n",
    "\n",
    "    def update_from_episode(self, episode, winner_id):\n",
    "        \"\"\"\n",
    "        episode is list of (state_key, action_idx, player_id).\n",
    "        winner_id is 0, 1, or None for draw.\n",
    "        \"\"\"\n",
    "        for state, action, pid in episode:\n",
    "            if winner_id is None:\n",
    "                G = 0.0\n",
    "            elif winner_id == pid:\n",
    "                G = 1.0\n",
    "            else:\n",
    "                G = -1.0\n",
    "\n",
    "            self._ensure_state(state)\n",
    "            self.N[state][action] += 1\n",
    "            n = self.N[state][action]\n",
    "            alpha = 1.0 / n\n",
    "            old = self.Q[state][action]\n",
    "            self.Q[state][action] = old + alpha * (G - old)\n",
    "\n",
    "    def play_self_game(self, epsilon):\n",
    "        board = EMPTY * 9\n",
    "        player_id = 0\n",
    "        episode = []\n",
    "\n",
    "        while True:\n",
    "            mark = MARKS[player_id]\n",
    "            move, state_key, a_can = self.choose_action(board, mark, epsilon)\n",
    "\n",
    "            # apply move\n",
    "            board = board[:move] + mark + board[move + 1:]\n",
    "\n",
    "            episode.append((state_key, a_can, player_id))\n",
    "\n",
    "            winner = check_winner(board)\n",
    "            if winner is not None:\n",
    "                if winner == \"draw\":\n",
    "                    winner_id = None\n",
    "                else:\n",
    "                    winner_id = MARKS.index(winner)\n",
    "                return episode, winner_id\n",
    "\n",
    "            player_id = 1 - player_id\n",
    "\n",
    "    def train_self_play(self, games=500, eps_start=0.3, eps_end=0.0):\n",
    "        for g in range(games):\n",
    "            # linear epsilon schedule\n",
    "            if games > 1:\n",
    "                epsilon = eps_start + (eps_end - eps_start) * (g / (games - 1))\n",
    "            else:\n",
    "                epsilon = eps_end\n",
    "\n",
    "            episode, winner_id = self.play_self_game(epsilon)\n",
    "            self.update_from_episode(episode, winner_id)\n",
    "\n",
    "    # simple play-vs-agent helper\n",
    "    def best_move(self, board, mark):\n",
    "        \"\"\"Deterministic greedy move from learned Q.\"\"\"\n",
    "        canon, mapping = canonical(board, mark)\n",
    "        self._ensure_state(canon)\n",
    "        avail_can = [i for i, c in enumerate(canon) if c == EMPTY]\n",
    "        qvals = self.Q[canon]\n",
    "        a_can = max(avail_can, key=lambda a: qvals[a])\n",
    "        return mapping[a_can]\n",
    "\n",
    "\n",
    "# ---------- demo ----------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(0)\n",
    "\n",
    "    agent = TicTacToeLearner()\n",
    "    agent.train_self_play(games=500, eps_start=0.3, eps_end=0.0)\n",
    "\n",
    "    # quick sanity check: have the agent play both sides once\n",
    "    def play_det_game(agent):\n",
    "        board = EMPTY * 9\n",
    "        player_id = 0\n",
    "        while True:\n",
    "            mark = MARKS[player_id]\n",
    "            move = agent.best_move(board, mark)\n",
    "            board = board[:move] + mark + board[move + 1:]\n",
    "            w = check_winner(board)\n",
    "            if w is not None:\n",
    "                return board, w\n",
    "            player_id = 1 - player_id\n",
    "\n",
    "    final_board, w = play_det_game(agent)\n",
    "    print(\"Final board:\", final_board[0:3], final_board[3:6], final_board[6:9])\n",
    "    print(\"Result:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959839bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
